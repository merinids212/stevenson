<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=cover">
<title>Stevenson — About</title>
<meta name="description" content="An art discovery engine that crawls painting listings from Craigslist and eBay, scores them with dual computer vision pipelines, and serves a curated gallery.">
<meta property="og:title" content="Stevenson — About">
<meta property="og:description" content="An art discovery engine that crawls painting listings from Craigslist and eBay, scores them with dual computer vision pipelines.">
<meta property="og:image" content="/og.jpg">
<meta name="twitter:card" content="summary_large_image">
<link rel="icon" type="image/svg+xml" href="favicon.svg">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;500;600&display=swap" rel="stylesheet">
<link rel="stylesheet" href="stv-nav.css">
<style>
  *, *::before, *::after {
    box-sizing: border-box;
    margin: 0;
    padding: 0;
  }

  :root {
    --font: 'IBM Plex Mono', monospace;
    --black: #000;
    --white: #fff;
    --gray: #999;
    --ease: cubic-bezier(0.4, 0, 0.2, 1);
  }

  html, body {
    height: 100%;
    font-family: var(--font);
    background: var(--white);
    color: var(--black);
    -webkit-font-smoothing: antialiased;
  }

  ::-webkit-scrollbar { width: 0; height: 0; }

  .about-content {
    max-width: 560px;
    margin: 0 auto;
    padding: calc(48px + env(safe-area-inset-top, 0px) + 60px) 24px 80px;
  }

  .about-content h1 {
    font-size: 14px;
    font-weight: 500;
    letter-spacing: 0.05em;
    text-transform: uppercase;
    margin-bottom: 40px;
  }

  .about-content h2 {
    font-size: 11px;
    font-weight: 600;
    letter-spacing: 0.08em;
    text-transform: uppercase;
    color: var(--gray);
    margin-top: 48px;
    margin-bottom: 16px;
  }

  .about-content p {
    font-size: 13px;
    line-height: 1.8;
    color: #333;
    margin-bottom: 24px;
  }

  .about-content code {
    font-family: var(--font);
    font-size: 12px;
    background: #f5f5f5;
    padding: 1px 5px;
    border: 1px solid #eee;
  }

  .about-content .stat {
    display: flex;
    justify-content: space-between;
    padding: 12px 0;
    border-bottom: 1px solid #f0f0f0;
    font-size: 12px;
    text-transform: uppercase;
    letter-spacing: 0.03em;
  }

  .about-content .stat-value {
    color: var(--gray);
  }

  .about-stats {
    margin-top: 48px;
    margin-bottom: 48px;
  }

</style>
</head>
<body>

<nav class="stv-nav" data-stv-theme="light" data-stv-page="about">
  <div class="stv-nav-center">
    <span class="stv-nav-title">About</span>
  </div>
</nav>

<div class="about-content">
  <h1>Stevenson</h1>

  <p>
    An art discovery engine that continuously crawls painting listings
    from Craigslist and eBay across all 50 US states, scores them with
    two independent computer vision pipelines, enriches them with
    subject/mood/color metadata, and serves a curated gallery &mdash;
    surfacing the most interesting work from the most unexpected places.
  </p>

  <p>
    No accounts, no fees, no middlemen. Just paintings and the
    people selling them.
  </p>

  <div class="about-stats" id="stats"></div>

  <h2>Scoring Pipeline 1: CLIP + LAION</h2>

  <p>
    Every painting is processed by a CLIP ViT-L/14 vision transformer
    to extract a 768-dimensional embedding. These embeddings are scored
    by a LAION aesthetic predictor trained on millions of human aesthetic
    preference ratings, calibrated from the raw [3, 8] output range to
    a 0&ndash;100 scale.
  </p>

  <p>
    Two additional signals are derived from the same embeddings.
    Quality measures cosine similarity to reference embeddings of
    professionally photographed fine art. Uniqueness measures how
    dissimilar a painting is from everything else in its scoring
    chunk, computed via an N&times;N cosine distance matrix. The
    composite art score blends all three:
    <code>LAION&times;0.5 + quality&times;0.3 + uniqueness&times;0.2</code>.
  </p>

  <h2>Scoring Pipeline 2: TOPIQ + MUSIQ</h2>

  <p>
    An independent second opinion. TOPIQ-IAA is a CFANet-based model
    trained on image aesthetic assessment, outputting mean opinion
    scores on a 1&ndash;5 scale. MUSIQ is a multi-scale image quality
    transformer trained on the AVA dataset &mdash; 250,000+ photographs
    rated by amateur photographers &mdash; outputting scores on a
    1&ndash;10 scale. Both are normalized to 0&ndash;100 and blended
    50/50 into an <code>aesthetic2</code> score.
  </p>

  <p>
    The two pipelines are trained on different data, optimize for
    different objectives, and disagree on ~30% of paintings. The
    analytics dashboard plots art score against aesthetic2 &mdash;
    paintings where both agree are safe bets; paintings where they
    disagree are the interesting ones worth investigating.
  </p>

  <h2>Enrichment</h2>

  <p>
    After scoring, a second pass enriches each painting with structured
    metadata. CLIP zero-shot classification runs the image against
    prompt dictionaries for 10 subjects (portrait, landscape, abstract,
    seascape, still life, etc.), 6 moods (serene, dramatic, mysterious,
    melancholy, joyful, romantic), and 6 mediums (oil, watercolor,
    acrylic, pastel, ink, mixed media). Each tag carries a confidence
    score from softmax over the prompt set.
  </p>

  <p>
    Dominant colors are extracted via MiniBatchKMeans clustering on the
    downsampled image (5 clusters, 100&times;100px). Each cluster center
    maps to a hex value, RGB triple, and percentage of image area. The
    clusters are then categorized into named colors (red, blue, teal,
    etc.) and tone tags (warm, cool, dark, light, muted, vivid) using
    HSL-space hue ranges and saturation/lightness thresholds.
  </p>

  <h2>Search</h2>

  <p>
    Search operates in CLIP's embedding space, not on keywords. Over
    200 terms &mdash; subjects, styles, moods, colors, art movements,
    compositions, artist style references &mdash; are pre-encoded with
    CLIP's text encoder into the same 768-dimensional space as the
    paintings. Typing "stormy impressionist seascape" finds paintings
    that look like that description, even if the listing title just
    says "old painting $40."
  </p>

  <h2>Taste Learning</h2>

  <p>
    The feed learns from your likes in real time. As you heart paintings,
    the system builds a taste profile from the CLIP embeddings of
    your liked works. When you've liked enough paintings, the algorithm
    identifies distinct "poles" in your taste &mdash; maybe you're drawn
    to both moody seascapes and vibrant street scenes &mdash; and queries
    each pole separately, ensuring your feed stays diverse rather than
    collapsing into a single aesthetic.
  </p>

  <p>
    Style affinity and price range are used as light signals &mdash;
    gentle nudges within small windows, never hard filters &mdash; so
    the feed feels personalized without becoming a filter bubble.
  </p>

  <h2>Gem Finding</h2>

  <p>
    A value score identifies underpriced work &mdash; paintings with high
    aesthetic quality relative to their asking price. The GEMS filter
    surfaces these finds: technically accomplished paintings that
    happen to be listed cheaply, often by sellers who don't know
    what they have.
  </p>

  <h2>The Taste Map</h2>

  <p>
    All 768-dimensional embeddings are projected into three dimensions
    using UMAP (Uniform Manifold Approximation and Projection), a
    manifold learning technique that preserves the local neighborhood
    structure of high-dimensional data. The result is a navigable 3D
    point cloud where similar paintings cluster together &mdash; oil
    landscapes in one region, abstract acrylics in another, portraits
    forming their own galaxy. Your liked paintings appear as bright
    red dots, revealing the shape of your taste in embedding space.
  </p>

  <h2>Pipeline</h2>

  <p>
    The crawlers are generator-based &mdash; each region or query
    yields batches of listings as they arrive, and scoring begins
    immediately on the first batch while scraping continues in
    parallel. Four models run in sequence per painting: CLIP ViT-L/14,
    LAION aesthetic MLP, TOPIQ-IAA, and MUSIQ-AVA. eBay is scraped
    via headless Chromium with randomized sort orders, page offsets,
    and human-like timing jitter to avoid detection.
  </p>

</div>

<script>
  fetch('/api/stats')
    .then(r => r.json())
    .then(s => {
      const stats = [
        ['Total listings', parseInt(s.total_listings).toLocaleString()],
        ['Sources', 'Craigslist + eBay'],
        ['Coverage', '50 US states'],
        ['Price range', `$${parseFloat(s.price_min).toLocaleString()} \u2014 $${parseFloat(s.price_max).toLocaleString()}`],
        ['Median price', `$${parseFloat(s.price_median).toLocaleString()}`],
        ['Vision scored', parseInt(s.scored_count).toLocaleString()],
        ['Artists identified', parseInt(s.artists_count).toLocaleString()],
        ['Scoring models', 'CLIP ViT-L/14 + LAION + TOPIQ + MUSIQ'],
      ];

      const container = document.getElementById('stats');
      stats.forEach(([label, value]) => {
        container.innerHTML += `<div class="stat"><span>${label}</span><span class="stat-value">${value}</span></div>`;
      });
    });
</script>
<script src="stv-nav.js"></script>
</body>
</html>
